{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dependency Parsing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you take a look at these material first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture6.pdf\n",
    "* http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf\n",
    "* https://github.com/rguthrie3/DeepDependencyParsingProblemSet/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2140537:140631287732032,MainProcess):2023-07-20-11:53:26.274.513 [mindspore/run_check/_check_version.py:102] MindSpore version 2.0.0.20230623 and cuda version 11.7.60 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, Tensor, ops, Parameter\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import os\n",
    "from IPython.display import display\n",
    "from mindnlp.modules import Accumulator\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "# 设置使用哪些显卡进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w]\n",
    "                    if word2index.get(w) is not None\n",
    "                    else word2index[\"<UNK>\"], seq))\n",
    "    sequence = Tensor(idxs, dtype=mindspore.int64)\n",
    "    return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition State Class "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tracks transition state(current stack, buffer) and extracts its feature for neural dependancy parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/05.transition-based-parse.png\">\n",
    "<center>borrowed image from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture6.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionState(object):\n",
    "\n",
    "    def __init__(self, tagged_sent):\n",
    "        self.root = ('ROOT', '<root>', -1)\n",
    "        self.stack = [self.root]\n",
    "        self.buffer = [(s[0], s[1], i) for i, s in enumerate(tagged_sent)]\n",
    "        self.address = [s[0] for s in tagged_sent] + [self.root[0]]\n",
    "        self.arcs = []\n",
    "        self.terminal = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'stack : %s \\nbuffer : %s' % (str([s[0] for s in self.stack]), str([b[0] for b in self.buffer]))\n",
    "\n",
    "    def shift(self):\n",
    "\n",
    "        if len(self.buffer) >= 1:\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        else:\n",
    "            print(\"Empty buffer\")\n",
    "\n",
    "    def left_arc(self, relation=None):\n",
    "\n",
    "        if len(self.stack) >= 2:\n",
    "            arc = {}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s1[0]\n",
    "            arc['addr'] = s1[2]\n",
    "            arc['head'] = s2[2]\n",
    "            arc['pos'] = s1[1]\n",
    "            if relation:\n",
    "                arc['relation'] = relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-2)\n",
    "\n",
    "        elif self.stack == [self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "\n",
    "    def right_arc(self, relation=None):\n",
    "\n",
    "        if len(self.stack) >= 2:\n",
    "            arc = {}\n",
    "            s2 = self.stack[-2]\n",
    "            s1 = self.stack[-1]\n",
    "            arc['graph_id'] = len(self.arcs)\n",
    "            arc['form'] = s2[0]\n",
    "            arc['addr'] = s2[2]\n",
    "            arc['head'] = s1[2]\n",
    "            arc['pos'] = s2[1]\n",
    "            if relation:\n",
    "                arc['relation'] = relation\n",
    "            self.arcs.append(arc)\n",
    "            self.stack.pop(-1)\n",
    "\n",
    "        elif self.stack == [self.root]:\n",
    "            print(\"Element Lacking\")\n",
    "\n",
    "    def get_left_most(self, index):\n",
    "        left = ['<NULL>', '<NULL>', None]\n",
    "\n",
    "        if index is None:\n",
    "            return left\n",
    "        for arc in self.arcs:\n",
    "            if arc['head'] == index:\n",
    "                left = [arc['form'], arc['pos'], arc['addr']]\n",
    "                break\n",
    "        return left\n",
    "\n",
    "    def get_right_most(self, index):\n",
    "        right = ['<NULL>', '<NULL>', None]\n",
    "\n",
    "        if index is None:\n",
    "            return right\n",
    "        for arc in reversed(self.arcs):\n",
    "            if arc['head'] == index:\n",
    "                right = [arc['form'], arc['pos'], arc['addr']]\n",
    "                break\n",
    "        return right\n",
    "\n",
    "    def is_done(self):\n",
    "        return len(self.buffer) == 0 and self.stack == [self.root]\n",
    "\n",
    "    def to_tree_string(self):\n",
    "        if self.is_done() is False:\n",
    "            return None\n",
    "        ingredient = []\n",
    "        for arc in self.arcs:\n",
    "            ingredient.append([arc['form'], self.address[arc['head']]])\n",
    "        ingredient = ingredient[-1:] + ingredient[:-1]\n",
    "        return self._make_tree(ingredient, 0)\n",
    "\n",
    "    def _make_tree(self, ingredient, i, new=True):\n",
    "\n",
    "        if new:\n",
    "            treestr = \"(\"\n",
    "            treestr += ingredient[i][0]\n",
    "            treestr += \" \"\n",
    "        else:\n",
    "            treestr = \"\"\n",
    "        ingredient[i][0] = \"CHECK\"\n",
    "\n",
    "        parents, _ = list(zip(*ingredient))\n",
    "\n",
    "        if ingredient[i][1] not in parents:\n",
    "            treestr += ingredient[i][1]\n",
    "            return treestr\n",
    "\n",
    "        else:\n",
    "            treestr += \"(\"\n",
    "            treestr += ingredient[i][1]\n",
    "            treestr += \" \"\n",
    "            for node_i, node in enumerate(parents):\n",
    "                if node == ingredient[i][1]:\n",
    "                    treestr += self._make_tree(ingredient, node_i, False)\n",
    "                    treestr += \" \"\n",
    "\n",
    "            treestr = treestr.strip()\n",
    "            treestr += \")\"\n",
    "        if new:\n",
    "            treestr += \")\"\n",
    "        return treestr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of transition-based dependancy parsing in the paper. Model's goal is to predict correct transition of parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack : ['ROOT'] \n",
      "buffer : ['He', 'has', 'good', 'control', '.']\n",
      "stack : ['ROOT', 'He', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['good', 'control', '.']\n",
      "[{'graph_id': 0, 'form': 'has', 'addr': 1, 'head': 0, 'pos': 'VBZ'}]\n",
      "stack : ['ROOT', 'has', 'good', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has', 'control'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : ['.']\n",
      "stack : ['ROOT', 'has'] \n",
      "buffer : []\n",
      "stack : ['ROOT'] \n",
      "buffer : []\n",
      "[{'graph_id': 0, 'form': 'has', 'addr': 1, 'head': 0, 'pos': 'VBZ'}, {'graph_id': 1, 'form': 'control', 'addr': 3, 'head': 2, 'pos': 'NN'}, {'graph_id': 2, 'form': 'has', 'addr': 1, 'head': 3, 'pos': 'VBZ'}, {'graph_id': 3, 'form': 'has', 'addr': 1, 'head': 4, 'pos': 'VBZ'}, {'graph_id': 4, 'form': 'ROOT', 'addr': -1, 'head': 1, 'pos': '<root>'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "state = TransitionState(nltk.pos_tag(\"He has good control .\".split()))\n",
    "print(state)\n",
    "state.shift()\n",
    "state.shift()\n",
    "print(state)\n",
    "state.left_arc()\n",
    "print(state)\n",
    "print(state.arcs)\n",
    "state.shift()\n",
    "state.shift()\n",
    "print(state)\n",
    "state.left_arc()\n",
    "print(state)\n",
    "state.right_arc()\n",
    "print(state)\n",
    "state.shift()\n",
    "state.right_arc()\n",
    "print(state)\n",
    "state.right_arc()\n",
    "print(state)\n",
    "print(state.arcs)\n",
    "state.is_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT (has He (control good) .))'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.to_tree_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,128.0,168.0\" width=\"128px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ROOT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">has</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">He</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"56.25%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">control</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">good</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.125%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"18.75%\" x=\"81.25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.625%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('ROOT', [Tree('has', ['He', Tree('control', ['good']), '.'])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = Tree.fromstring(state.to_tree_string())\n",
    "display(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & Preprocessing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get features from stack and buffer. <br>\n",
    "1.  The top 3 words on the stack and buffer ($s_1,s_2,s_3,b_1,b_2,b_3$)\n",
    "2. The first and second leftmost / rightmost children of the top two words on the stack: $lc_1(s_i), rc_1(s_i), lTc_2(s_i), rc_2(s_i), i = 1, 2$\n",
    "3. <i>The leftmost of leftmost / rightmost of rightmost children of the top two words on the stack: $lc_1(lc_1(s_i)), rc_1(rc_1(s_i)), i = 1, 2$. # I don't use these features</i>\n",
    "4. POS tags for $S^t$\n",
    "5. <i>corresponding arc labels of words excluding those 6 words on the stack/buffer for $S^l$ # I don't use these features</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat(transition_state, word2index, tag2index, label2index=None):\n",
    "    word_feats = []\n",
    "    tag_feats = []\n",
    "\n",
    "    word_feats.append(transition_state.stack[-1][0]) if len(transition_state.stack) >= 1 and \\\n",
    "        transition_state.stack[-1][0] in word2index.keys() else word_feats.append('<NULL>')  # s1\n",
    "    word_feats.append(transition_state.stack[-2][0]) if len(transition_state.stack) >= 2 and \\\n",
    "        transition_state.stack[-2][0] in word2index.keys() else word_feats.append('<NULL>')  # s2\n",
    "    word_feats.append(transition_state.stack[-3][0]) if len(transition_state.stack) >= 3 and \\\n",
    "        transition_state.stack[-3][0] in word2index.keys() else word_feats.append('<NULL>')  # s3\n",
    "\n",
    "    tag_feats.append(transition_state.stack[-1][1]) if len(transition_state.stack) >= 1 and \\\n",
    "        transition_state.stack[-1][1] in tag2index.keys() else tag_feats.append('<NULL>')  # st1\n",
    "    tag_feats.append(transition_state.stack[-2][1]) if len(transition_state.stack) >= 2 and \\\n",
    "        transition_state.stack[-2][1] in tag2index.keys() else tag_feats.append('<NULL>')  # st2\n",
    "    tag_feats.append(transition_state.stack[-3][1]) if len(transition_state.stack) >= 3 and \\\n",
    "        transition_state.stack[-3][1] in tag2index.keys() else tag_feats.append('<NULL>')  # st3\n",
    "\n",
    "\n",
    "    word_feats.append(transition_state.buffer[0][0]) if len(transition_state.buffer) >= 1 and \\\n",
    "        transition_state.buffer[0][0] in word2index.keys() else word_feats.append('<NULL>')  # b1\n",
    "    word_feats.append(transition_state.buffer[1][0]) if len(transition_state.buffer) >= 2 and \\\n",
    "        transition_state.buffer[1][0] in word2index.keys() else word_feats.append('<NULL>')  # b2\n",
    "    word_feats.append(transition_state.buffer[2][0]) if len(transition_state.buffer) >= 3 and \\\n",
    "        transition_state.buffer[2][0] in word2index.keys() else word_feats.append('<NULL>')  # b3\n",
    "\n",
    "    tag_feats.append(transition_state.buffer[0][1]) if len(transition_state.buffer) >= 1 and \\\n",
    "        transition_state.buffer[0][1] in tag2index.keys() else tag_feats.append('<NULL>')  # bt1\n",
    "    tag_feats.append(transition_state.buffer[1][1]) if len(transition_state.buffer) >= 2 and \\\n",
    "        transition_state.buffer[1][1] in tag2index.keys() else tag_feats.append('<NULL>')  # bt2\n",
    "    tag_feats.append(transition_state.buffer[2][1]) if len(transition_state.buffer) >= 3 and \\\n",
    "        transition_state.buffer[2][1] in tag2index.keys() else tag_feats.append('<NULL>')  # bt3\n",
    "\n",
    "\n",
    "    lc_s1 = transition_state.get_left_most(transition_state.stack[-1][2]) if len(transition_state.stack) >= 1 \\\n",
    "        else transition_state.get_left_most(None)\n",
    "    rc_s1 = transition_state.get_right_most(transition_state.stack[-1][2]) if len(transition_state.stack) >= 1 \\\n",
    "        else transition_state.get_right_most(None)\n",
    "\n",
    "    lc_s2 = transition_state.get_left_most(transition_state.stack[-2][2]) if len(transition_state.stack) >= 2 \\\n",
    "        else transition_state.get_left_most(None)\n",
    "    rc_s2 = transition_state.get_right_most(transition_state.stack[-2][2]) if len(transition_state.stack) >= 2 \\\n",
    "        else transition_state.get_right_most(None)\n",
    "\n",
    "    words, tags, _ = zip(*[lc_s1, rc_s1, lc_s2, rc_s2])\n",
    "\n",
    "    word_feats.extend(words)\n",
    "\n",
    "    tag_feats.extend(tags)\n",
    "\n",
    "\n",
    "    return prepare_sequence(word_feats, word2index).view(1, -1), prepare_sequence(tag_feats, tag2index).view(1, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get data from <a href=\"https://github.com/rguthrie3/DeepDependencyParsingProblemSet\">this repo</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../dataset/dparser/train.txt', 'r').readlines()\n",
    "vocab = open('../dataset/dparser/vocab.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splited_data = [[nltk.pos_tag(d.split('|||')[0].split()), d.split('|||')[1][:-1].split()] for d in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = list(zip(*splited_data))\n",
    "train_x_f = flatten(train_x)\n",
    "sents, pos_tags = list(zip(*train_x_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2index = {v: i for i, v in enumerate(set(pos_tags))}\n",
    "tag2index['<root>'] = len(tag2index)\n",
    "tag2index['<NULL>'] = len(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [v.split('\\t')[0] for v in vocab]\n",
    "word2index = {v: i for i, v in enumerate(vocab)}\n",
    "word2index['ROOT'] = len(word2index)\n",
    "word2index['<NULL>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = ['SHIFT', 'REDUCE_L', 'REDUCE_R']\n",
    "action2index = {v: i for i, v in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tx, ty in splited_data:\n",
    "    state = TransitionState(tx)\n",
    "    transition = ty + ['REDUCE_R']  # root\n",
    "    while len(transition):\n",
    "        feat = get_feat(state, word2index, tag2index)\n",
    "        action = transition.pop(0)\n",
    "        actionTensor = Parameter(Tensor([action2index[action]], dtype=mindspore.int64)).view(1, -1)\n",
    "        train_data.append([feat, actionTensor])\n",
    "        if action == 'SHIFT':\n",
    "            state.shift()\n",
    "        elif action == 'REDUCE_R':\n",
    "            state.right_arc()\n",
    "        elif action == 'REDUCE_L':\n",
    "            state.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Tensor(shape=[1, 10], dtype=Int64, value=\n",
       "  [[9151, 9152, 9152 ... 9152, 9152, 9152]]),\n",
       "  Tensor(shape=[1, 10], dtype=Int64, value=\n",
       "  [[43, 44, 44 ... 44, 44, 44]])),\n",
       " Tensor(shape=[1, 1], dtype=Int64, value=\n",
       " [[0]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/05.neural-dparser-architecture.png\">\n",
    "<center>borrowed image from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture6.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralDependencyParser(nn.Cell):\n",
    "\n",
    "    def __init__(self, w_size, w_embed_dim, t_size, t_embed_dim, hidden_size, target_size):\n",
    "\n",
    "        super(NeuralDependencyParser, self).__init__()\n",
    "\n",
    "        self.w_embed = nn.Embedding(w_size, w_embed_dim)\n",
    "        self.t_embed = nn.Embedding(t_size, t_embed_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.target_size = target_size\n",
    "        self.dense = nn.Dense((w_embed_dim + t_embed_dim) * 10, self.hidden_size)\n",
    "        self.out = nn.Dense(self.hidden_size, self.target_size)\n",
    "\n",
    "        minval = Tensor(-0.01, mindspore.float32)\n",
    "        maxval = Tensor(0.01, mindspore.float32)\n",
    "        self.w_embed.embedding_table.set_data(ops.uniform(self.w_embed.embedding_table.shape, minval, maxval))  # init\n",
    "        self.t_embed.embedding_table.set_data(ops.uniform(self.t_embed.embedding_table.shape, minval, maxval))  # init\n",
    "\n",
    "    def construct(self, words, tags):\n",
    "\n",
    "        wem = self.w_embed(words).view(words.shape[0], -1)\n",
    "        tem = self.t_embed(tags).view(tags.shape[0], -1)\n",
    "        inputs = ops.cat([wem, tem], 1)\n",
    "        h1 = ops.pow(self.dense(inputs), 3)  # cube activation function\n",
    "        preds = -self.out(h1)\n",
    "        return ops.log_softmax(preds, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 5\n",
    "BATCH_SIZE = 256\n",
    "W_EMBED_SIZE = 50\n",
    "T_EMBED_SIZE = 10\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NeuralDependencyParser(len(word2index), W_EMBED_SIZE, len(tag2index), T_EMBED_SIZE, HIDDEN_SIZE, len(action2index))\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accumulate_step = 2\n",
    "accumulator = Accumulator(optimizer, accumulate_step)\n",
    "\n",
    "\n",
    "def forward_fn(words, tags, targets):\n",
    "    \"\"\"Forward function\"\"\"\n",
    "    preds = model(words, tags)\n",
    "    targets = targets.astype(mindspore.int32)\n",
    "    loss = loss_function(preds, targets.view(-1))\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "@mindspore.jit\n",
    "def train_step(words, tags, targets):\n",
    "    \"\"\"Training steps\"\"\"\n",
    "    loss, grads = grad_fn(words, tags, targets)\n",
    "    loss = ops.depend(loss, accumulator(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.10\n",
      "mean_loss : 1.09\n",
      "mean_loss : 0.80\n",
      "mean_loss : 0.46\n",
      "mean_loss : 0.35\n",
      "mean_loss : 0.30\n",
      "mean_loss : 0.29\n",
      "mean_loss : 0.27\n",
      "mean_loss : 0.25\n",
      "mean_loss : 0.24\n",
      "mean_loss : 0.22\n",
      "mean_loss : 0.22\n",
      "mean_loss : 0.21\n",
      "mean_loss : 0.21\n",
      "mean_loss : 0.20\n",
      "mean_loss : 0.20\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "    inputs, targets = list(zip(*batch))\n",
    "    words, tags = list(zip(*inputs))\n",
    "    words = ops.cat(words)\n",
    "    tags = ops.cat(tags)\n",
    "    targets = ops.cat(targets)\n",
    "    loss = train_step(words, tags, targets)\n",
    "\n",
    "    losses.append(loss.asnumpy().item(0) * accumulate_step)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"mean_loss : %0.2f\" % (np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (UAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = open('../dataset/dparser/dev.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splited_data = [[nltk.pos_tag(d.split('|||')[0].split()), d.split('|||')[1][:-1].split()] for d in dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tx, ty in splited_data:\n",
    "    state = TransitionState(tx)\n",
    "    transition = ty + ['REDUCE_R']  # root\n",
    "    while len(transition) != 0:\n",
    "        feat = get_feat(state, word2index, tag2index)\n",
    "        action = transition.pop(0)\n",
    "        dev_data.append([feat, action2index[action]])\n",
    "        if action == 'SHIFT':\n",
    "            state.shift()\n",
    "        elif action == 'REDUCE_R':\n",
    "            state.right_arc()\n",
    "        elif action == 'REDUCE_L':\n",
    "            state.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.25930521091811\n"
     ]
    }
   ],
   "source": [
    "for dev in dev_data:\n",
    "    input, target = dev[0], dev[1]\n",
    "    word, tag = input[0], input[1]\n",
    "    pred = ops.max(model(word, tag), 1)[1]\n",
    "    pred = pred.asnumpy().item(0)\n",
    "    if pred == target:\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy / len(dev_data) * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting parsed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = TransitionState(nltk.pos_tag(\"I shot an elephant in my pajamas\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2action = {i: v for v, i in action2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while test.is_done() is False:\n",
    "    feat = get_feat(test, word2index, tag2index)\n",
    "    word, tag = feat[0], feat[1]\n",
    "    action = ops.max(model(word, tag), 1)[1]\n",
    "    action = action.asnumpy().item(0)\n",
    "\n",
    "    action = index2action[action]\n",
    "\n",
    "    if action == 'SHIFT':\n",
    "        test.shift()\n",
    "    elif action == 'REDUCE_R':\n",
    "        test.right_arc()\n",
    "    elif action == 'REDUCE_L':\n",
    "        test.left_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack : ['ROOT'] \n",
      "buffer : []\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT (shot I (elephant an) (in (pajamas my))))'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.to_tree_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,176.0,216.0\" width=\"176px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ROOT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">shot</text></svg><svg width=\"13.6364%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">I</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.81818%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"45.4545%\" x=\"13.6364%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">elephant</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">an</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.3636%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40.9091%\" x=\"59.0909%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pajamas</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">my</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5455%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('ROOT', [Tree('shot', ['I', Tree('elephant', ['an']), Tree('in', [Tree('pajamas', ['my'])])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = Tree.fromstring(test.to_tree_string())\n",
    "display(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://arxiv.org/pdf/1506.06158.pdf\">Structured Training for Neural Network Transition-Based Parsing</a>\n",
    "* <a href=\"https://arxiv.org/pdf/1703.04474\">DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
