{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skip-gram with negative sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you take a look at these material first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture3.pdf\n",
    "* http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2126545:139879694522176,MainProcess):2023-07-20-10:29:20.508.10 [mindspore/run_check/_check_version.py:102] MindSpore version 2.0.0.20230623 and cuda version 11.7.60 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore\n",
    "from mindspore import nn, Tensor, ops\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from mindnlp.modules import Accumulator\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0.20230623\n",
      "3.7\n"
     ]
    }
   ],
   "source": [
    "print(mindspore.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "# 设置使用哪些显卡进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w]\n",
    "                    if word2index.get(w) is not None\n",
    "                    else word2index[\"<UNK>\"], seq))\n",
    "    sequence = Tensor(idxs, dtype=mindspore.int64)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Tensor([word2index[word]], dtype=mindspore.int64) \\\n",
    "        if word2index.get(word) is not None \\\n",
    "        else Tensor([word2index[\"<UNK>\"]], dtype=mindspore.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude sparse words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 3\n",
    "exclude = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w, c in word_count.items():\n",
    "    if c < MIN_COUNT:\n",
    "        exclude.append(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)) - set(exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if window[i] in exclude or window[WINDOW_SIZE] in exclude:\n",
    "            continue  # min_count\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "\n",
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))\n",
    "\n",
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50242"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Unigram Distribution**0.75 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w)=U(w)^{3/4}/Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo] / num_total_words)**0.75) / Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 3500\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab), len(unigram_table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].asnumpy().item(0)\n",
    "        while len(nsample) < k:  # num of sampling\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "\n",
    "    return ops.cat(neg_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/02.skipgram-objective.png\">\n",
    "<center>borrowed image from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture3.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Cell):\n",
    "\n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)  # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)  # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "        initrange = (2.0 / (vocab_size + projection_dim))**0.5  # Xavier init\n",
    "        minval = Tensor(-initrange, mindspore.float32)\n",
    "        maxval = Tensor(initrange, mindspore.float32)\n",
    "        self.embedding_v.embedding_table.set_data(ops.uniform(self.embedding_v.embedding_table.shape, minval, maxval))  # init\n",
    "        self.embedding_u.embedding_table.set_data(ops.zeros(self.embedding_u.embedding_table.shape, mindspore.float32))  # init\n",
    "\n",
    "    def construct(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words)  # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words)  # B x 1 x D\n",
    "\n",
    "        neg_embeds = -self.embedding_u(negative_words)  # B x K x D\n",
    "\n",
    "        positive_score = ops.BatchMatMul()(target_embeds, ops.transpose(center_embeds, (0, 2, 1))).squeeze(2)  # Bx1\n",
    "        negative_score = ops.sum(ops.BatchMatMul()(neg_embeds, ops.transpose(center_embeds, (0, 2, 1))).squeeze(2), 1).view(negs.shape[0], -1)  # BxK -> Bx1\n",
    "\n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "\n",
    "        return -ops.mean(loss)\n",
    "\n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "\n",
    "        return embeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 101\n",
    "NEG = 10  # Num of Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_step = 2\n",
    "accumulator = Accumulator(optimizer, accumulate_step)\n",
    "\n",
    "\n",
    "def forward_fn(inputs, targets, negs):\n",
    "    \"\"\"Forward function\"\"\"\n",
    "    loss = model(inputs, targets, negs)\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "@mindspore.jit\n",
    "def train_step(inputs, targets, negs):\n",
    "    \"\"\"Training steps\"\"\"\n",
    "    loss, grads = grad_fn(inputs, targets, negs)\n",
    "    loss = ops.depend(loss, accumulator(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 1.17\n",
      "Epoch : 10, mean_loss : 0.87\n",
      "Epoch : 20, mean_loss : 0.83\n",
      "Epoch : 30, mean_loss : 0.78\n",
      "Epoch : 40, mean_loss : 0.74\n",
      "Epoch : 50, mean_loss : 0.72\n",
      "Epoch : 60, mean_loss : 0.70\n",
      "Epoch : 70, mean_loss : 0.68\n",
      "Epoch : 80, mean_loss : 0.67\n",
      "Epoch : 90, mean_loss : 0.66\n",
      "Epoch : 100, mean_loss : 0.65\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "\n",
    "        inputs, targets = zip(*batch)\n",
    "\n",
    "        inputs = ops.cat(inputs)  # B x 1\n",
    "        targets = ops.cat(targets)  # B x 1\n",
    "        negs = negative_sampling(targets, unigram_table, NEG)\n",
    "\n",
    "        loss = train_step(inputs, targets, negs)\n",
    "\n",
    "        losses.append(loss.asnumpy().item(0) * accumulate_step)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target:\n",
    "            continue\n",
    "\n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "\n",
    "        cosine_sim = ops.cosine_similarity(target_V, vector).asnumpy().tolist()[0]\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nantucket', 0.7158238291740417],\n",
       " ['yet', 0.6215175986289978],\n",
       " ['behind', 0.6064300537109375],\n",
       " ['man', 0.5916519165039062],\n",
       " ['without', 0.5414537191390991],\n",
       " ['whale', 0.5358570218086243],\n",
       " ['bed', 0.5276554226875305],\n",
       " ['craft', 0.5193333625793457],\n",
       " ['who', 0.5097337365150452],\n",
       " ['being', 0.5067386031150818]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
