{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Neural Machine Translation and Models with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you take a look at these material first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf\n",
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf\n",
    "* https://arxiv.org/pdf/1409.0473.pdf\n",
    "* https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983\n",
    "* http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1103492:139665709524800,MainProcess):2023-07-09-11:39:09.399.390 [mindspore/run_check/_check_version.py:102] MindSpore version 2.0.0.20230623 and cuda version 11.7.60 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, Tensor, ops, Parameter\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from mindnlp.modules import Accumulator\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "from mindspore.common.initializer import initializer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "random.seed(1024)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "# 设置使用哪些显卡进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/07.pad_to_sequence.png\">\n",
    "<center>borrowed image from https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is for Sequence 2 Sequence format\n",
    "def pad_to_batch(batch, x_to_ix, y_to_ix):\n",
    "\n",
    "    sorted_batch = sorted(batch, key=lambda b: b[0].shape[1], reverse=True)  # sort by len\n",
    "    x, y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.shape[1] for s in x])\n",
    "    max_y = max([s.shape[1] for s in y])\n",
    "    x_p, y_p = [], []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].shape[1] < max_x:\n",
    "            x_p.append(ops.cat([x[i], Parameter(Tensor([x_to_ix['<PAD>']] * (max_x - x[i].shape[1]), dtype=mindspore.int64)).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].shape[1] < max_y:\n",
    "            y_p.append(ops.cat([y[i], Parameter(Tensor([y_to_ix['<PAD>']] * (max_y - y[i].shape[1]), dtype=mindspore.int64)).view(1, -1)], 1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "\n",
    "    input_var = ops.cat(x_p)\n",
    "    target_var = ops.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s == 0, t.asnumpy())).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s == 0, t.asnumpy())).count(False) for t in target_var]\n",
    "\n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w]\n",
    "                    if word2index.get(w) is not None\n",
    "                    else word2index[\"<UNK>\"], seq))\n",
    "    sequence = Tensor(idxs, dtype=mindspore.int64)\n",
    "    return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>French -> English</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = open('../dataset/eng-fra.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217975"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus[:30000]  # for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29459 29459\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n",
      "CPU times: user 817 ms, sys: 20.7 ms, total: 837 ms\n",
      "Wall time: 837 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = [], []  # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so, ta, _ = parallel[:-1].split('\\t')\n",
    "    if so.strip() == \"\" or ta.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "\n",
    "    if len(normalized_so) >= MIN_LENGTH and len(normalized_so) <= MAX_LENGTH \\\n",
    "        and len(normalized_ta) >= MIN_LENGTH \\\n",
    "            and len(normalized_ta) <= MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "\n",
    "\n",
    "print(len(X_r), len(y_r))\n",
    "print(X_r[0], y_r[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4266 7439\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab), len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in source_vocab:\n",
    "    if source2index.get(vo) is None:\n",
    "        source2index[vo] = len(source2index)\n",
    "index2source = {v: k for k, v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in target_vocab:\n",
    "    if target2index.get(vo) is None:\n",
    "        target2index[vo] = len(target2index)\n",
    "index2target = {v: k for k, v in target2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.12 s, sys: 1.23 s, total: 9.34 s\n",
      "Wall time: 6.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p, y_p = [], []\n",
    "\n",
    "for so, ta in zip(X_r, y_r):\n",
    "    X_p.append(prepare_sequence(so + ['</s>'], source2index).view(1, -1))\n",
    "    y_p.append(prepare_sequence(ta + ['</s>'], target2index).view(1, -1))\n",
    "\n",
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/07.seq2seq.png\">\n",
    "<center>borrowd image from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not familier with <strong>pack_padded_sequence</strong> and <strong>pad_packed_sequence</strong>, check this <a href=\"https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983\">post</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1, bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, embedding_table=\"XavierUniform\")\n",
    "\n",
    "        if bidirec:\n",
    "            self.n_direction = 2\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "    def init_hidden(self, inputs):\n",
    "        hidden = Parameter(ops.zeros((self.n_layers * self.n_direction, inputs.shape[0], self.hidden_size)))\n",
    "        return hidden\n",
    "\n",
    "    def init_weight(self, embedding_size, hidden_size):\n",
    "        self.gru.weight_ih = initializer('XavierUniform', [embedding_size, hidden_size])\n",
    "        self.gru.weight_hh = initializer('XavierUniform', [embedding_size, hidden_size])\n",
    "\n",
    "    def construct(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B, T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        embedded = self.embedding(inputs)\n",
    "        input_lengths = Tensor(input_lengths).astype(mindspore.int32)\n",
    "\n",
    "        outputs, hidden = self.gru(embedded, hidden, input_lengths)\n",
    "\n",
    "        if self.n_layers > 1:\n",
    "            if self.n_direction == 2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        h_last = ops.cat(list(hidden), 1)\n",
    "        h_last = h_last.unsqueeze(1)\n",
    "        return outputs, h_last"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism ( https://arxiv.org/pdf/1409.0473.pdf )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used general-type for score function $h_t^TW_ah_s^-$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/07.attention-mechanism.png\">\n",
    "<center>borrowed image from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size, embedding_table=\"XavierUniform\")\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size + hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.dense = nn.Dense(hidden_size * 2, input_size, weight_init=\"XavierUniform\")\n",
    "        self.attn = nn.Dense(self.hidden_size, self.hidden_size, weight_init=\"XavierUniform\")  # Attention\n",
    "\n",
    "    def init_hidden(self, inputs):\n",
    "        hidden = Parameter(ops.zeros((self.n_layers, inputs.shape[0], self.hidden_size)))\n",
    "        return hidden\n",
    "\n",
    "    def init_weight(self, embedding_size, hidden_size):\n",
    "        self.gru.weight_ih = initializer('XavierUniform', [embedding_size, hidden_size])\n",
    "        self.gru.weight_hh = initializer('XavierUniform', [embedding_size, hidden_size])\n",
    "\n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)   # (1,B,D) -> (B,D,1)\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]  # B\n",
    "        max_len = encoder_outputs.shape[1]  # T\n",
    "        energies = self.attn(encoder_outputs.view(batch_size * max_len, -1))  # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size, max_len, -1)  # B,T,D\n",
    "        attn_energies = ops.BatchMatMul()(energies, hidden).squeeze(2)  # B,T,D * B,D,1 --> B,T\n",
    "\n",
    "        alpha = ops.softmax(attn_energies, 1)  # B,T\n",
    "        alpha = alpha.unsqueeze(1)  # B,1,T\n",
    "        context = ops.BatchMatMul()(alpha, encoder_outputs)  # B,1,T * B,T,D => B,1,D\n",
    "\n",
    "        return context, alpha\n",
    "\n",
    "    def construct(self, inputs, context, max_length, encoder_outputs, encoder_maskings=None, is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "\n",
    "        decode = []\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(ops.cat((embedded, context), 2), hidden)  # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = ops.cat((hidden, ops.transpose(context, (1, 0, 2))), 2)  # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.dense(concated.squeeze(0))\n",
    "            softmaxed = ops.log_softmax(score, 1)\n",
    "            softmaxed = Tensor(softmaxed, dtype=mindspore.float32)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = ops.max(softmaxed, 1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1)  # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "\n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs, encoder_maskings)\n",
    "\n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = ops.cat(decode, 1)\n",
    "        return scores.view(inputs.shape[0] * max_length, -1)\n",
    "\n",
    "    def decode(self, context, encoder_outputs):\n",
    "        start_decode = Parameter(ops.transpose(Tensor([[target2index['<s>']] * 1], dtype=mindspore.int64), (1, 0)))\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "\n",
    "        decodes = []\n",
    "        attentions = []\n",
    "        decoded = embedded\n",
    "        while decoded.asnumpy().item(0) != target2index['</s>']:  # until </s>\n",
    "            _, hidden = self.gru(ops.cat((embedded, context), 2), hidden)  # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = ops.cat((hidden, ops.transpose(context, (1, 0, 2))), 2)  # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.dense(concated.squeeze(0))\n",
    "            softmaxed = ops.log_softmax(score, 1)\n",
    "            softmaxed = Tensor(softmaxed, dtype=mindspore.float32)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = ops.max(softmaxed, 1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1)  # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs, None)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "\n",
    "        return ops.max(ops.cat(decodes), 1)[1], ops.cat(attentions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while if you use just cpu...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 0.001\n",
    "DECODER_LEARNING_RATIO = 5.0\n",
    "RESCHEDULED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2index), EMBEDDING_SIZE, HIDDEN_SIZE, 3, True)\n",
    "decoder = Decoder(len(target2index), EMBEDDING_SIZE, HIDDEN_SIZE * 2)\n",
    "encoder.init_weight(EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "decoder.init_weight(EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = nn.Adam(encoder.trainable_params(), learning_rate=LR)\n",
    "dec_optimizer = nn.Adam(decoder.trainable_params(), learning_rate=LR * DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_step = 2\n",
    "accumulator_encoder = Accumulator(enc_optimizer, accumulate_step)\n",
    "accumulator_decoder = Accumulator(dec_optimizer, accumulate_step)\n",
    "\n",
    "\n",
    "def forward_fn(batch, is_training):\n",
    "    \"\"\"Forward function\"\"\"\n",
    "    inputs, targets, input_lengths, target_lengths = pad_to_batch(batch, source2index, target2index)\n",
    "\n",
    "    input_masks = ops.cat([Tensor(tuple(map(lambda s: s == 0, t.asnumpy())), dtype=mindspore.byte) for t in inputs]).view(inputs.shape[0], -1)\n",
    "    start_decode = ops.transpose(Tensor([[target2index['<s>']] * targets.shape[0]], dtype=mindspore.int64), (1, 0))\n",
    "\n",
    "    output, hidden_c = encoder(inputs, input_lengths)\n",
    "    preds = decoder(start_decode, hidden_c, targets.shape[1], output, input_masks, is_training)\n",
    "    targets = targets.astype(mindspore.int32)\n",
    "    loss = loss_function(preds, targets.view(-1))\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn_encoder = mindspore.value_and_grad(forward_fn, None, encoder.trainable_params())\n",
    "grad_fn_decoder = mindspore.value_and_grad(forward_fn, None, decoder.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "def train_step(batch, is_training):\n",
    "    \"\"\"Training steps\"\"\"\n",
    "    loss, grads_encoder = grad_fn_encoder(batch, is_training)\n",
    "    loss = ops.depend(loss, accumulator_encoder(grads_encoder))\n",
    "    grads_encoder = ops.clip_by_value(grads_encoder, clip_value_min=-50.0, clip_value_max=50.0)\n",
    "    loss, grads_decoder = grad_fn_decoder(batch, is_training)\n",
    "    loss = ops.depend(loss, accumulator_decoder(grads_decoder))\n",
    "    grads_decoder = ops.clip_by_value(grads_decoder, clip_value_min=-50.0, clip_value_max=50.0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [000/460] mean_loss : 8.92\n",
      "[00/50] [200/460] mean_loss : 4.58\n",
      "[00/50] [400/460] mean_loss : 3.59\n",
      "[01/50] [000/460] mean_loss : 3.09\n",
      "[01/50] [200/460] mean_loss : 2.97\n",
      "[01/50] [400/460] mean_loss : 2.84\n",
      "[02/50] [000/460] mean_loss : 2.30\n",
      "[02/50] [200/460] mean_loss : 2.40\n",
      "[02/50] [400/460] mean_loss : 2.37\n",
      "[03/50] [000/460] mean_loss : 1.72\n",
      "[03/50] [200/460] mean_loss : 2.08\n",
      "[03/50] [400/460] mean_loss : 2.05\n",
      "[04/50] [000/460] mean_loss : 1.64\n",
      "[04/50] [200/460] mean_loss : 1.71\n",
      "[04/50] [400/460] mean_loss : 1.75\n",
      "[05/50] [000/460] mean_loss : 1.61\n",
      "[05/50] [200/460] mean_loss : 1.51\n",
      "[05/50] [400/460] mean_loss : 1.64\n",
      "[06/50] [000/460] mean_loss : 1.22\n",
      "[06/50] [200/460] mean_loss : 1.37\n",
      "[06/50] [400/460] mean_loss : 1.53\n",
      "[07/50] [000/460] mean_loss : 1.06\n",
      "[07/50] [200/460] mean_loss : 1.26\n",
      "[07/50] [400/460] mean_loss : 1.34\n",
      "[08/50] [000/460] mean_loss : 1.18\n",
      "[08/50] [200/460] mean_loss : 1.14\n",
      "[08/50] [400/460] mean_loss : 1.25\n",
      "[09/50] [000/460] mean_loss : 0.99\n",
      "[09/50] [200/460] mean_loss : 1.06\n",
      "[09/50] [400/460] mean_loss : 1.16\n",
      "[10/50] [000/460] mean_loss : 0.99\n",
      "[10/50] [200/460] mean_loss : 1.00\n",
      "[10/50] [400/460] mean_loss : 1.11\n",
      "[11/50] [000/460] mean_loss : 1.12\n",
      "[11/50] [200/460] mean_loss : 0.96\n",
      "[11/50] [400/460] mean_loss : 1.06\n",
      "[12/50] [000/460] mean_loss : 0.83\n",
      "[12/50] [200/460] mean_loss : 0.93\n",
      "[12/50] [400/460] mean_loss : 1.02\n",
      "[13/50] [000/460] mean_loss : 0.81\n",
      "[13/50] [200/460] mean_loss : 0.89\n",
      "[13/50] [400/460] mean_loss : 0.98\n",
      "[14/50] [000/460] mean_loss : 0.72\n",
      "[14/50] [200/460] mean_loss : 0.85\n",
      "[14/50] [400/460] mean_loss : 0.92\n",
      "[15/50] [000/460] mean_loss : 0.85\n",
      "[15/50] [200/460] mean_loss : 0.82\n",
      "[15/50] [400/460] mean_loss : 0.90\n",
      "[16/50] [000/460] mean_loss : 0.89\n",
      "[16/50] [200/460] mean_loss : 0.84\n",
      "[16/50] [400/460] mean_loss : 0.95\n",
      "[17/50] [000/460] mean_loss : 0.72\n",
      "[17/50] [200/460] mean_loss : 0.79\n",
      "[17/50] [400/460] mean_loss : 0.89\n",
      "[18/50] [000/460] mean_loss : 0.74\n",
      "[18/50] [200/460] mean_loss : 0.76\n",
      "[18/50] [400/460] mean_loss : 0.96\n",
      "[19/50] [000/460] mean_loss : 0.58\n",
      "[19/50] [200/460] mean_loss : 0.84\n",
      "[19/50] [400/460] mean_loss : 0.89\n",
      "[20/50] [000/460] mean_loss : 0.69\n",
      "[20/50] [200/460] mean_loss : 0.78\n",
      "[20/50] [400/460] mean_loss : 0.85\n",
      "[21/50] [000/460] mean_loss : 0.55\n",
      "[21/50] [200/460] mean_loss : 0.74\n",
      "[21/50] [400/460] mean_loss : 0.81\n",
      "[22/50] [000/460] mean_loss : 0.67\n",
      "[22/50] [200/460] mean_loss : 0.71\n",
      "[22/50] [400/460] mean_loss : 0.80\n",
      "[23/50] [000/460] mean_loss : 0.61\n",
      "[23/50] [200/460] mean_loss : 0.68\n",
      "[23/50] [400/460] mean_loss : 0.76\n",
      "[24/50] [000/460] mean_loss : 0.82\n",
      "[24/50] [200/460] mean_loss : 0.70\n",
      "[24/50] [400/460] mean_loss : 0.76\n",
      "[25/50] [000/460] mean_loss : 0.81\n",
      "[25/50] [200/460] mean_loss : 0.67\n",
      "[25/50] [400/460] mean_loss : 0.76\n",
      "[26/50] [000/460] mean_loss : 0.61\n",
      "[26/50] [200/460] mean_loss : 0.62\n",
      "[26/50] [400/460] mean_loss : 0.60\n",
      "[27/50] [000/460] mean_loss : 0.61\n",
      "[27/50] [200/460] mean_loss : 0.57\n",
      "[27/50] [400/460] mean_loss : 0.56\n",
      "[28/50] [000/460] mean_loss : 0.46\n",
      "[28/50] [200/460] mean_loss : 0.54\n",
      "[28/50] [400/460] mean_loss : 0.53\n",
      "[29/50] [000/460] mean_loss : 0.54\n",
      "[29/50] [200/460] mean_loss : 0.52\n",
      "[29/50] [400/460] mean_loss : 0.52\n",
      "[30/50] [000/460] mean_loss : 0.63\n",
      "[30/50] [200/460] mean_loss : 0.49\n",
      "[30/50] [400/460] mean_loss : 0.50\n",
      "[31/50] [000/460] mean_loss : 0.43\n",
      "[31/50] [200/460] mean_loss : 0.49\n",
      "[31/50] [400/460] mean_loss : 0.48\n",
      "[32/50] [000/460] mean_loss : 0.53\n",
      "[32/50] [200/460] mean_loss : 0.48\n",
      "[32/50] [400/460] mean_loss : 0.47\n",
      "[33/50] [000/460] mean_loss : 0.36\n",
      "[33/50] [200/460] mean_loss : 0.46\n",
      "[33/50] [400/460] mean_loss : 0.46\n",
      "[34/50] [000/460] mean_loss : 0.48\n",
      "[34/50] [200/460] mean_loss : 0.45\n",
      "[34/50] [400/460] mean_loss : 0.45\n",
      "[35/50] [000/460] mean_loss : 0.50\n",
      "[35/50] [200/460] mean_loss : 0.44\n",
      "[35/50] [400/460] mean_loss : 0.44\n",
      "[36/50] [000/460] mean_loss : 0.50\n",
      "[36/50] [200/460] mean_loss : 0.43\n",
      "[36/50] [400/460] mean_loss : 0.43\n",
      "[37/50] [000/460] mean_loss : 0.40\n",
      "[37/50] [200/460] mean_loss : 0.42\n",
      "[37/50] [400/460] mean_loss : 0.42\n",
      "[38/50] [000/460] mean_loss : 0.34\n",
      "[38/50] [200/460] mean_loss : 0.41\n",
      "[38/50] [400/460] mean_loss : 0.43\n",
      "[39/50] [000/460] mean_loss : 0.46\n",
      "[39/50] [200/460] mean_loss : 0.41\n",
      "[39/50] [400/460] mean_loss : 0.40\n",
      "[40/50] [000/460] mean_loss : 0.31\n",
      "[40/50] [200/460] mean_loss : 0.40\n",
      "[40/50] [400/460] mean_loss : 0.40\n",
      "[41/50] [000/460] mean_loss : 0.34\n",
      "[41/50] [200/460] mean_loss : 0.40\n",
      "[41/50] [400/460] mean_loss : 0.40\n",
      "[42/50] [000/460] mean_loss : 0.41\n",
      "[42/50] [200/460] mean_loss : 0.39\n",
      "[42/50] [400/460] mean_loss : 0.39\n",
      "[43/50] [000/460] mean_loss : 0.41\n",
      "[43/50] [200/460] mean_loss : 0.39\n",
      "[43/50] [400/460] mean_loss : 0.39\n",
      "[44/50] [000/460] mean_loss : 0.34\n",
      "[44/50] [200/460] mean_loss : 0.38\n",
      "[44/50] [400/460] mean_loss : 0.39\n",
      "[45/50] [000/460] mean_loss : 0.36\n",
      "[45/50] [200/460] mean_loss : 0.38\n",
      "[45/50] [400/460] mean_loss : 0.38\n",
      "[46/50] [000/460] mean_loss : 0.37\n",
      "[46/50] [200/460] mean_loss : 0.37\n",
      "[46/50] [400/460] mean_loss : 0.38\n",
      "[47/50] [000/460] mean_loss : 0.44\n",
      "[47/50] [200/460] mean_loss : 0.38\n",
      "[47/50] [400/460] mean_loss : 0.37\n",
      "[48/50] [000/460] mean_loss : 0.31\n",
      "[48/50] [200/460] mean_loss : 0.37\n",
      "[48/50] [400/460] mean_loss : 0.37\n",
      "[49/50] [000/460] mean_loss : 0.39\n",
      "[49/50] [200/460] mean_loss : 0.37\n",
      "[49/50] [400/460] mean_loss : 0.36\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "\n",
    "        loss = train_step(batch, True)\n",
    "        losses.append(loss.asnumpy().item(0) * accumulate_step)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" % (epoch, EPOCH, i, len(train_data) // BATCH_SIZE, np.mean(losses)))\n",
    "            losses = []\n",
    "\n",
    "    if RESCHEDULED is False and epoch == EPOCH // 2:\n",
    "        LR *= 0.01\n",
    "        enc_optimizer = nn.Adam(encoder.trainable_params(), learning_rate=LR)\n",
    "        dec_optimizer = nn.Adam(decoder.trainable_params(), learning_rate=LR * DECODER_LEARNING_RATIO)\n",
    "        accumulator_encoder = Accumulator(enc_optimizer, accumulate_step)\n",
    "        accumulator_decoder = Accumulator(dec_optimizer, accumulate_step)\n",
    "        RESCHEDULED = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.asnumpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  i m in no hurry .\n",
      "Truth :  je ne suis pas presse .\n",
      "Prediction :  je ne suis pas presse .\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1].view(-1)\n",
    "\n",
    "output, hidden = encoder(input_, [input_.shape[1]])\n",
    "pred, attn = decoder.decode(hidden, output)\n",
    "\n",
    "input_ = input_.view(-1)\n",
    "input_ = [index2source[i] for i in input_.asnumpy()]\n",
    "pred = [index2target[i] for i in pred.asnumpy()]\n",
    "\n",
    "print('Source : ', ' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ', ' '.join([index2target[i] for i in truth.asnumpy() if i not in [2, 3]]))\n",
    "print('Prediction : ', ' '.join([i for i in pred if i not in ['</s>']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_attention(input_, pred, attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU\n",
    "* Beam Search\n",
    "* <a href=\"http://www.aclweb.org/anthology/P15-1001\">Sampled Softmax</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf\">Convolutional Sequence to Sequence learning</a>\n",
    "* <a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>\n",
    "* <a href=\"https://arxiv.org/abs/1711.00043\">Unsupervised Machine Translation Using Monolingual Corpora Only</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Suggested Reading "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://arxiv.org/pdf/1709.07809.pdf\">SMT chapter13. Neural Machine Translation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
