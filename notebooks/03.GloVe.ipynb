{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GloVe: Global Vectors for Word Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you take a look at these material first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture3.pdf\n",
    "* https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2144502:140133311055680,MainProcess):2023-07-20-12:14:08.268.998 [mindspore/run_check/_check_version.py:102] MindSpore version 2.0.0.20230623 and cuda version 11.7.60 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
      "/home/daiyuxin/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore\n",
    "from mindspore import nn, ops, Tensor, Parameter\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from mindnlp.modules import Accumulator\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0.20230623\n",
      "3.7\n"
     ]
    }
   ],
   "source": [
    "print(mindspore.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = '0'\n",
    "# 设置使用哪些显卡进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w]\n",
    "                    if word2index.get(w) is not None\n",
    "                    else word2index[\"<UNK>\"], seq))\n",
    "    sequence = Tensor(idxs, dtype=mindspore.int64)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Tensor([word2index[word]], dtype=mindspore.int64) \\\n",
    "        if word2index.get(word) is not None \\\n",
    "        else Tensor([word2index[\"<UNK>\"]], dtype=mindspore.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "\n",
    "window_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
    "            continue\n",
    "        window_data.append((window[WINDOW_SIZE], window[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/03.glove-weighting-function.png\">\n",
    "<center>borrowed image from https://nlp.stanford.edu/pubs/glove.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j):\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except Exception:\n",
    "        x_ij = 1\n",
    "\n",
    "    x_max = 100  # 100 fixed in paper\n",
    "    alpha = 0.75\n",
    "\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    else:\n",
    "        result = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Co-occurence Matrix X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of model complexity, It is important to determine whether a tighter bound can be placed on the number of nonzero elements of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_i = Counter(flatten(corpus))  # X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ik_window_5 = Counter(window_data)  # Co-occurece in window size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ik = {}\n",
    "weighting_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_window_5.get(bigram) is not None:  # nonzero elements\n",
    "        co_occer = X_ik_window_5[bigram]\n",
    "        X_ik[bigram] = co_occer + 1  # log(Xik) -> log(Xik+1) to prevent divergence\n",
    "        X_ik[(bigram[1], bigram[0])] = co_occer + 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1])\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sacred', 'any')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(window_data)\n",
    "print(test)\n",
    "try:\n",
    "    print(X_ik[(test[0], test[1])] == X_ik[(test[1], test[0])])\n",
    "except Exception:\n",
    "    1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1, 1], dtype=Int64, value=\n",
      "[[144]]), Tensor(shape=[1, 1], dtype=Int64, value=\n",
      "[[2169]]), Tensor(shape=[1, 1], dtype=Float32, value=\n",
      "[[ 6.93147182e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=\n",
      "[[ 5.31829596e-02]]))\n"
     ]
    }
   ],
   "source": [
    "u_p = []  # center vec\n",
    "v_p = []  # context vec\n",
    "co_p = []  # log(x_ij)\n",
    "weight_p = []  # f(x_ij)\n",
    "\n",
    "for pair in window_data:\n",
    "    u_p.append(prepare_word(pair[0], word2index).view(1, -1))\n",
    "    v_p.append(prepare_word(pair[1], word2index).view(1, -1))\n",
    "\n",
    "    try:\n",
    "        cooc = X_ik[pair]\n",
    "    except Exception:\n",
    "        cooc = 1\n",
    "\n",
    "    co_p.append(ops.log(Parameter(Tensor([cooc], dtype=mindspore.float32))).view(1, -1))\n",
    "    weight_p.append(Parameter(Tensor([weighting_dic[pair]], dtype=mindspore.float32)).view(1, -1))\n",
    "\n",
    "train_data = list(zip(u_p, v_p, co_p, weight_p))\n",
    "del u_p\n",
    "del v_p\n",
    "del co_p\n",
    "del weight_p\n",
    "print(train_data[0])  # tuple (center vec i, context vec j log(x_ij), weight f(w_ij))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/03.glove-objective.png\">\n",
    "<center>borrowed image from https://nlp.stanford.edu/pubs/glove.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Cell):\n",
    "\n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)  # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)  # out embedding\n",
    "\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "        initrange = (2.0 / (vocab_size + projection_dim))**0.5  # Xavier init\n",
    "        minval = Tensor(-initrange, mindspore.float32)\n",
    "        maxval = Tensor(initrange, mindspore.float32)\n",
    "        self.embedding_v.embedding_table.set_data(ops.uniform(self.embedding_v.embedding_table.shape, minval, maxval))  # init\n",
    "        self.embedding_u.embedding_table.set_data(ops.uniform(self.embedding_u.embedding_table.shape, minval, maxval))  # init\n",
    "        self.v_bias.embedding_table.set_data(ops.uniform(self.v_bias.embedding_table.shape, minval, maxval))  # init\n",
    "        self.u_bias.embedding_table.set_data(ops.uniform(self.u_bias.embedding_table.shape, minval, maxval))  # init\n",
    "\n",
    "    def construct(self, center_words, target_words, coocs, weights):\n",
    "        center_embeds = self.embedding_v(center_words)  # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words)  # B x 1 x D\n",
    "\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        inner_product = ops.BatchMatMul()(target_embeds, ops.transpose(center_embeds, (0, 2, 1))).squeeze(2)  # Bx1\n",
    "\n",
    "        loss = weights * ops.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "\n",
    "        return ops.sum(loss)\n",
    "\n",
    "    def prediction(self, inputs):\n",
    "        v_embeds = self.embedding_v(inputs)  # B x 1 x D\n",
    "        u_embeds = self.embedding_u(inputs)  # B x 1 x D\n",
    "\n",
    "        return v_embeds + u_embeds  # final embed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = GloVe(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_step = 2\n",
    "accumulator = Accumulator(optimizer, accumulate_step)\n",
    "\n",
    "\n",
    "def forward_fn(inputs, targets, coocs, weights):\n",
    "    \"\"\"Forward function\"\"\"\n",
    "    loss = model(inputs, targets, coocs, weights)\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "@mindspore.jit\n",
    "def train_step(inputs, targets, coocs, weights):\n",
    "    \"\"\"Training steps\"\"\"\n",
    "    loss, grads = grad_fn(inputs, targets, coocs, weights)\n",
    "    loss = ops.depend(loss, accumulator(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 379.39\n",
      "Epoch : 10, mean_loss : 5.88\n",
      "Epoch : 20, mean_loss : 1.22\n",
      "Epoch : 30, mean_loss : 0.51\n",
      "Epoch : 40, mean_loss : 0.20\n",
      "Epoch : 50, mean_loss : 0.08\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "\n",
    "        inputs, targets, coocs, weights = zip(*batch)\n",
    "        inputs = ops.cat(inputs)  # B x 1\n",
    "        targets = ops.cat(targets)  # B x 1\n",
    "        coocs = ops.cat(coocs)\n",
    "        weights = ops.cat(weights)\n",
    "\n",
    "        loss = train_step(inputs, targets, coocs, weights)\n",
    "        losses.append(loss.asnumpy().item() * accumulate_step)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target:\n",
    "            continue\n",
    "\n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "\n",
    "        cosine_sim = ops.cosine_similarity(target_V, vector).asnumpy().tolist()[0]\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surely'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rev', 0.9097740650177002],\n",
       " ['nick', 0.8757970929145813],\n",
       " ['trod', 0.8692667484283447],\n",
       " [',', 0.8691204786300659],\n",
       " ['breaches', 0.8660611510276794],\n",
       " ['henry', 0.8610873222351074],\n",
       " ['flail', 0.8497377038002014],\n",
       " ['sharks', 0.8434864282608032],\n",
       " ['and', 0.8433796763420105],\n",
       " ['artificial', 0.8418195843696594]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(test, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use <a href=\"https://docs.scipy.org/doc/scipy/reference/sparse.html\">sparse-matrix</a> to build co-occurence matrix for memory efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Readings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"http://ruder.io/word-embeddings-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI\">Word embeddings in 2017: Trends and future directions</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
